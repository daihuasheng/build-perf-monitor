"""
Standalone Command-Line Tool for Generating Plots from MyMonitor Data.

This script reads monitoring data files (.parquet) generated by mymonitor and
creates interactive time-series plots using Plotly. It can be run automatically
after a monitoring session or manually by a user to re-analyze or customize plots.

Key Features:
- Discovers all `.parquet` data files in a given log directory.
- Filters data files based on project name and parallelism level.
- Provides flexible options to filter data by category or top memory consumers.
- Allows customization of plot types and time-series resampling intervals.
- Saves plots as interactive HTML files and, if Kaleido is installed, as
  static PNG images.
"""

import argparse
import logging
import re
import sys
from pathlib import Path
from typing import List, Optional

# Third-party library imports
import polars as pl
import plotly.express as px
import plotly.graph_objects as go

# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger("PlotterTool")

# --- Module Constants ---

# A threshold (in KB) to filter out categories with insignificant memory usage
# from the plots, keeping them clean and focused. 10MB = 10 * 1024 KB.
MIN_TOTAL_PRIMARY_METRIC_KB_FOR_PLOT = 10240


# --- Helper Functions (largely unchanged from the original module) ---


def _get_primary_metric_from_summary_log(data_filepath: Path) -> Optional[str]:
    """
    Parses the corresponding _summary.log file to find the primary metric used.
    """
    if data_filepath.name.endswith("_summary.log"):
        return None

    summary_log_path = data_filepath.with_name(f"{data_filepath.stem}_summary.log")
    if not summary_log_path.exists():
        logger.warning(
            f"Could not find summary log: {summary_log_path} to determine primary metric. Falling back to RSS_KB."
        )
        return "RSS_KB"

    try:
        with open(summary_log_path, "r") as f:
            for line in f:
                match_peak = re.search(r"Peak Overall Memory \(([^)]+)\):", line)
                if match_peak:
                    metric = match_peak.group(1)
                    logger.info(
                        f"Determined primary metric '{metric}' from summary log (peak line)."
                    )
                    return metric
                match_collector = re.search(r"Memory Metric Collector: (\w+)", line)
                if match_collector:
                    collector_type_short = match_collector.group(1).lower()
                    if "pss" in collector_type_short:
                        return "PSS_KB"
                    elif "rss" in collector_type_short:
                        return "RSS_KB"
    except Exception as e:
        logger.error(
            f"Error reading or parsing summary log {summary_log_path}: {e}. Falling back to RSS_KB."
        )
    return "RSS_KB"


def _save_plotly_figure(fig: go.Figure, base_filename: str, output_dir: Path):
    """
    Saves a Plotly figure to both HTML and, if possible, PNG formats.
    """
    plot_filename_html = output_dir / f"{base_filename}.html"
    try:
        fig.write_html(plot_filename_html)
        logger.info(f"Interactive plot saved to: {plot_filename_html}")
        try:
            plot_filename_png = output_dir / f"{base_filename}.png"
            fig.write_image(plot_filename_png, width=1200, height=600)
            logger.info(f"Static plot saved to: {plot_filename_png}")
        except Exception as e_kaleido:
            logger.warning(
                f"Failed to save static plot to PNG (Kaleido might be missing or misconfigured): {e_kaleido}. "
                f"To enable PNG export, install Kaleido: `uv pip install mymonitor[export]`"
            )
    except Exception as e:
        logger.error(
            f"Failed to save plot {plot_filename_html} using Plotly: {e}",
            exc_info=True,
        )


def _generate_line_plot_plotly(
    df_plot_data: pl.DataFrame,
    primary_metric_col: str,
    resample_interval_str: str,
    data_filepath: Path,
    output_dir: Path,
):
    """
    Generates and saves an interactive line plot using Plotly.
    """
    if df_plot_data.is_empty():
        logger.warning(
            f"Line Plot: Input data is empty for {data_filepath.name}. Skipping."
        )
        return

    resampled_dfs_list = []
    for category_name_tuple, group_df in df_plot_data.group_by(
        "Category", maintain_order=True
    ):
        if group_df.is_empty():
            continue
        resampled_cat_df = (
            group_df.sort("Timestamp")
            .group_by_dynamic(
                index_column="Timestamp",
                every=resample_interval_str,
                group_by="Category",
            )
            .agg(pl.col(primary_metric_col).mean().alias(primary_metric_col))
            .fill_null(0)
        )
        if not resampled_cat_df.is_empty():
            resampled_dfs_list.append(resampled_cat_df)

    if not resampled_dfs_list:
        logger.warning(
            f"Line Plot: No data after resampling for {data_filepath.name}. Skipping."
        )
        return

    combined_resampled_df = pl.concat(resampled_dfs_list)
    if combined_resampled_df.is_empty():
        logger.warning(
            f"Line Plot: Combined resampled data is empty for {data_filepath.name}. Skipping."
        )
        return

    fig = px.line(
        combined_resampled_df.to_pandas(),
        x="Timestamp",
        y=primary_metric_col,
        color="Category",
        title=f"Memory Usage Over Time ({primary_metric_col} - Lines) - {data_filepath.stem}<br>Resample: {resample_interval_str}",
        labels={
            "Timestamp": f"Time (Resampled to {resample_interval_str})",
            primary_metric_col: f"Average {primary_metric_col} (KB)",
        },
        markers=True,
    )

    total_df = (
        combined_resampled_df.group_by("Timestamp")
        .agg(pl.col(primary_metric_col).sum().alias("Total_Memory"))
        .sort("Timestamp")
    )
    if not total_df.is_empty():
        fig.add_trace(
            go.Scatter(
                x=total_df["Timestamp"].to_list(),
                y=total_df["Total_Memory"].to_list(),
                mode="lines",
                name="Total (Sum of Categories)",
                line={"color": "black", "dash": "dash"},
            )
        )

    fig.update_layout(
        legend_title_text="Category",
        xaxis_title=f"Time (Resampled to {resample_interval_str} intervals)",
        yaxis_title=f"Average {primary_metric_col} Memory Usage (KB)",
    )

    base_plot_filename = f"{data_filepath.stem}_{primary_metric_col}_lines_plot"
    _save_plotly_figure(fig, base_plot_filename, output_dir)


def _generate_stacked_area_plot_plotly(
    df_plot_data: pl.DataFrame,
    primary_metric_col: str,
    resample_interval_str: str,
    data_filepath: Path,
    output_dir: Path,
):
    """
    Generates and saves an interactive stacked area plot using Plotly.
    """
    if df_plot_data.is_empty():
        logger.warning(
            f"Stacked Plot: Input data is empty for {data_filepath.name}. Skipping."
        )
        return

    resampled_df = (
        df_plot_data.sort("Timestamp")
        .group_by_dynamic(
            index_column="Timestamp",
            every=resample_interval_str,
            group_by="Category",
        )
        .agg(pl.col(primary_metric_col).mean().fill_null(0).alias(primary_metric_col))
    )

    if resampled_df.is_empty():
        logger.warning(
            f"Stacked Plot: Resampled data is empty for {data_filepath.name}. Skipping."
        )
        return

    fig = px.area(
        resampled_df.to_pandas(),
        x="Timestamp",
        y=primary_metric_col,
        color="Category",
        title=f"Memory Usage Over Time ({primary_metric_col} - Stacked Area) - {data_filepath.stem}<br>Resample: {resample_interval_str}",
        labels={
            "Timestamp": f"Time (Resampled to {resample_interval_str})",
            primary_metric_col: f"{primary_metric_col} (KB)",
        },
    )

    fig.update_layout(
        legend_title_text="Category",
        xaxis_title=f"Time (Resampled to {resample_interval_str} intervals)",
        yaxis_title=f"Total {primary_metric_col} Memory Usage (KB) - Stacked",
    )

    base_plot_filename = f"{data_filepath.stem}_{primary_metric_col}_stacked_plot"
    _save_plotly_figure(fig, base_plot_filename, output_dir)


# --- Main Worker Function ---


def plot_memory_usage_from_data_file(data_filepath: Path, args: argparse.Namespace):
    """
    Reads a Parquet file and generates plots based on provided arguments.
    """
    output_dir = args.output_dir or args.log_dir
    output_dir.mkdir(parents=True, exist_ok=True)

    primary_metric_col = _get_primary_metric_from_summary_log(data_filepath)
    if not primary_metric_col:
        logger.error(
            f"Could not determine primary metric for {data_filepath.name}. Skipping."
        )
        return

    try:
        df_pl = pl.read_parquet(data_filepath)
        if df_pl.is_empty():
            logger.warning(f"No data found in {data_filepath}. Skipping plot.")
            return

        df_per_process_pl = df_pl.filter(pl.col("Record_Type") == "PROCESS")
        if df_per_process_pl.is_empty():
            logger.warning(
                f"No per-process data rows in {data_filepath.name}. Skipping."
            )
            return

        df_per_process_pl = df_per_process_pl.with_columns(
            (pl.col("Major_Category") + "_" + pl.col("Minor_Category")).alias(
                "Category"
            )
        )

        required_cols = ["Timestamp_epoch", "Category", primary_metric_col]
        if not all(col in df_per_process_pl.columns for col in required_cols):
            missing = [c for c in required_cols if c not in df_per_process_pl.columns]
            logger.error(
                f"Data in {data_filepath} missing required columns: {missing}."
            )
            return

        df_per_process_pl = df_per_process_pl.with_columns(
            pl.col(primary_metric_col).cast(pl.Float64, strict=False)
        ).filter(pl.col(primary_metric_col).is_not_null())

        if df_per_process_pl.is_empty():
            logger.warning(
                f"No valid numeric data for '{primary_metric_col}' in {data_filepath.name}. Skipping."
            )
            return

        df_per_process_pl = df_per_process_pl.filter(
            ~pl.col("Category").str.starts_with("Ignored_")
        )
        if df_per_process_pl.is_empty():
            logger.warning("No data after filtering ignored categories. Skipping.")
            return

        # --- Custom Filtering based on CLI arguments ---
        if args.category:
            logger.info(f"Filtering for user-specified categories: {args.category}")
            df_per_process_pl = df_per_process_pl.filter(
                pl.col("Category").is_in(args.category)
            )
        elif args.top_n:
            logger.info(f"Filtering for top {args.top_n} categories by peak memory.")
            top_categories = (
                df_per_process_pl.group_by("Category")
                .agg(pl.col(primary_metric_col).max().alias("peak_mem"))
                .sort("peak_mem", descending=True)
                .head(args.top_n)["Category"]
                .to_list()
            )
            df_per_process_pl = df_per_process_pl.filter(
                pl.col("Category").is_in(top_categories)
            )
        else:
            # Default behavior: filter for significant categories
            category_total_metric = df_per_process_pl.group_by("Category").agg(
                pl.col(primary_metric_col).sum().alias("total_metric")
            )
            significant_categories = category_total_metric.filter(
                pl.col("total_metric") >= MIN_TOTAL_PRIMARY_METRIC_KB_FOR_PLOT
            )["Category"].to_list()
            df_per_process_pl = df_per_process_pl.filter(
                pl.col("Category").is_in(significant_categories)
            )

        if df_per_process_pl.is_empty():
            logger.warning(
                f"No data remains after filtering for {data_filepath.name}. Skipping plots."
            )
            return

        logger.info(
            f"Plotting for categories: {df_per_process_pl['Category'].unique().to_list()}"
        )

        df_plot_data_pl = df_per_process_pl.with_columns(
            pl.from_epoch("Timestamp_epoch", time_unit="s").alias("Timestamp")
        )

        # --- Resampling Interval Logic ---
        if args.resample_interval:
            resample_interval_str_polars = args.resample_interval
            logger.info(
                f"Using user-provided resample interval: {resample_interval_str_polars}"
            )
        else:
            min_time_dt = df_plot_data_pl["Timestamp"].min()
            max_time_dt = df_plot_data_pl["Timestamp"].max()
            duration_seconds = (
                (max_time_dt - min_time_dt).total_seconds()
                if min_time_dt and max_time_dt
                else 0
            )
            if duration_seconds <= 60:
                resample_interval_str_polars = "5s"
            elif duration_seconds < 300:
                resample_interval_str_polars = "10s"
            elif duration_seconds < 900:
                resample_interval_str_polars = "30s"
            else:
                resample_interval_str_polars = "1m"
            logger.info(
                f"Data duration: {duration_seconds:.0f}s. Using dynamic resample interval: {resample_interval_str_polars}"
            )

        # --- Plot Generation ---
        if args.chart_type in ["line", "all"]:
            _generate_line_plot_plotly(
                df_plot_data_pl,
                primary_metric_col,
                resample_interval_str_polars,
                data_filepath,
                output_dir,
            )
        if args.chart_type in ["stacked", "all"]:
            _generate_stacked_area_plot_plotly(
                df_plot_data_pl,
                primary_metric_col,
                resample_interval_str_polars,
                data_filepath,
                output_dir,
            )

    except pl.exceptions.NoDataError:
        logger.warning(
            f"No data in {data_filepath} (Polars NoDataError). Skipping plot."
        )
    except Exception as e:
        logger.error(f"Error generating plots for {data_filepath}: {e}", exc_info=True)


# --- Main CLI Entry Point ---


def main():
    """
    Main command-line interface function for the plotter tool.
    """
    parser = argparse.ArgumentParser(
        description="Generate plots from mymonitor data files.",
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument(
        "--log-dir",
        type=Path,
        required=True,
        help="Required. Path to the run-specific log directory containing .parquet and .log files.",
    )
    parser.add_argument(
        "--project-name",
        type=str,
        help="Filter to only generate plots for a specific project name.",
    )
    parser.add_argument(
        "--jobs",
        type=int,
        help="Filter to only generate plots for a specific parallelism level (-j N).",
    )
    parser.add_argument(
        "--chart-type",
        choices=["line", "stacked", "all"],
        default="all",
        help="Specify which chart types to generate. Default: all.",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        help="Directory to save plots. Defaults to the specified --log-dir.",
    )
    parser.add_argument(
        "--resample-interval",
        type=str,
        help="Override automatic resampling. Use Polars interval string (e.g., '1s', '10s', '1m').",
    )

    filter_group = parser.add_mutually_exclusive_group()
    filter_group.add_argument(
        "--category",
        type=str,
        action="append",
        help="Generate plots for only a specific category (e.g., 'Compiler_clang'). Can be specified multiple times.",
    )
    filter_group.add_argument(
        "--top-n",
        type=int,
        metavar="N",
        help="Only plot the top N categories by peak memory usage.",
    )

    args = parser.parse_args()

    if not args.log_dir.is_dir():
        logger.error(f"Log directory not found: {args.log_dir}")
        sys.exit(1)

    logger.info(f"Searching for Parquet data files in: {args.log_dir}")
    all_parquet_files = list(args.log_dir.glob("*.parquet"))

    if not all_parquet_files:
        logger.info(f"No Parquet data files found in {args.log_dir}.")
        return

    # Filter files based on project name and jobs if specified
    files_to_plot = []
    for f in all_parquet_files:
        filename = f.name
        if args.project_name and args.project_name not in filename:
            continue
        if args.jobs and f"_j{args.jobs}_" not in filename:
            continue
        files_to_plot.append(f)

    if not files_to_plot:
        logger.warning("No data files matched the specified filters.")
        return

    logger.info(f"Found {len(files_to_plot)} matching data file(s) to process.")
    for data_file in files_to_plot:
        logger.info(f"--- Generating plot for {data_file.name} ---")
        plot_memory_usage_from_data_file(data_file, args)


if __name__ == "__main__":
    main()
